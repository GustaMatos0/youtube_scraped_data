{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making all necessary imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>keywords</th>\n",
       "      <th>channel_links</th>\n",
       "      <th>video</th>\n",
       "      <th>date</th>\n",
       "      <th>length</th>\n",
       "      <th>views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I got the Fortnite Only Up WORLD RECORD! (Spee...</td>\n",
       "      <td>⬆️ PLAY MY ONLY UP MAP NOW!! ► 5264-1761-9807❤...</td>\n",
       "      <td>video, sharing, camera phone, video phone, fre...</td>\n",
       "      <td>https://www.youtube.com/@TGplays</td>\n",
       "      <td>https://www.youtube.com/watch?v=4HlBgHmknY4</td>\n",
       "      <td>2023-07-12T19:36:52-07:00</td>\n",
       "      <td>PT19M55S</td>\n",
       "      <td>1.2M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Ron DeSantis: It is important to stand for a c...</td>\n",
       "      <td>2024 GOP presidential candidate Gov. Ron DeSan...</td>\n",
       "      <td>DeSantis, Ron DeSantis, DeSantis abortion, abo...</td>\n",
       "      <td>https://www.youtube.com/@FoxNews</td>\n",
       "      <td>https://www.youtube.com/watch?v=pWpOn6C0YAk</td>\n",
       "      <td>2024-01-09T16:14:58-08:00</td>\n",
       "      <td>PT5M23S</td>\n",
       "      <td>26K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Game Theory: Viewers' Choice, Cyborgs, Fatalit...</td>\n",
       "      <td>Your voices have been heard! As thanks for sup...</td>\n",
       "      <td>Chrono Trigger, Mario, Super Mario, Illusion o...</td>\n",
       "      <td>https://www.youtube.com/@GameTheory</td>\n",
       "      <td>https://www.youtube.com/watch?v=z4QwsHsu3uw</td>\n",
       "      <td>2011-07-06T09:05:29-07:00</td>\n",
       "      <td>PT8M27S</td>\n",
       "      <td>958K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>C-R-O-W-N-E-D - Kirby's Return to Dream Land +...</td>\n",
       "      <td>MY LINKS:●Main channel: https://www.youtube.co...</td>\n",
       "      <td>music, extended, ost</td>\n",
       "      <td>https://www.youtube.com/@AacroXtensions</td>\n",
       "      <td>https://www.youtube.com/watch?v=iPx1YkOVGKE</td>\n",
       "      <td>2023-04-26T17:51:46-07:00</td>\n",
       "      <td>PT30M1S</td>\n",
       "      <td>1.2K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Tostarena: Night - Super Mario Odyssey Music E...</td>\n",
       "      <td>MY LINKS:●Main channel: https://www.youtube.co...</td>\n",
       "      <td>music, extended, ost</td>\n",
       "      <td>https://www.youtube.com/@AacroXtensions</td>\n",
       "      <td>https://www.youtube.com/watch?v=xYY8KI_00tY</td>\n",
       "      <td>2023-06-28T22:01:18-07:00</td>\n",
       "      <td>PT30M2S</td>\n",
       "      <td>3.1K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0           0  I got the Fortnite Only Up WORLD RECORD! (Spee...   \n",
       "1           1  Ron DeSantis: It is important to stand for a c...   \n",
       "2           2  Game Theory: Viewers' Choice, Cyborgs, Fatalit...   \n",
       "3           3  C-R-O-W-N-E-D - Kirby's Return to Dream Land +...   \n",
       "4           4  Tostarena: Night - Super Mario Odyssey Music E...   \n",
       "\n",
       "                                         description  \\\n",
       "0  ⬆️ PLAY MY ONLY UP MAP NOW!! ► 5264-1761-9807❤...   \n",
       "1  2024 GOP presidential candidate Gov. Ron DeSan...   \n",
       "2  Your voices have been heard! As thanks for sup...   \n",
       "3  MY LINKS:●Main channel: https://www.youtube.co...   \n",
       "4  MY LINKS:●Main channel: https://www.youtube.co...   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  video, sharing, camera phone, video phone, fre...   \n",
       "1  DeSantis, Ron DeSantis, DeSantis abortion, abo...   \n",
       "2  Chrono Trigger, Mario, Super Mario, Illusion o...   \n",
       "3                               music, extended, ost   \n",
       "4                               music, extended, ost   \n",
       "\n",
       "                             channel_links  \\\n",
       "0         https://www.youtube.com/@TGplays   \n",
       "1         https://www.youtube.com/@FoxNews   \n",
       "2      https://www.youtube.com/@GameTheory   \n",
       "3  https://www.youtube.com/@AacroXtensions   \n",
       "4  https://www.youtube.com/@AacroXtensions   \n",
       "\n",
       "                                         video                       date  \\\n",
       "0  https://www.youtube.com/watch?v=4HlBgHmknY4  2023-07-12T19:36:52-07:00   \n",
       "1  https://www.youtube.com/watch?v=pWpOn6C0YAk  2024-01-09T16:14:58-08:00   \n",
       "2  https://www.youtube.com/watch?v=z4QwsHsu3uw  2011-07-06T09:05:29-07:00   \n",
       "3  https://www.youtube.com/watch?v=iPx1YkOVGKE  2023-04-26T17:51:46-07:00   \n",
       "4  https://www.youtube.com/watch?v=xYY8KI_00tY  2023-06-28T22:01:18-07:00   \n",
       "\n",
       "     length views  \n",
       "0  PT19M55S  1.2M  \n",
       "1   PT5M23S   26K  \n",
       "2   PT8M27S  958K  \n",
       "3   PT30M1S  1.2K  \n",
       "4   PT30M2S  3.1K  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the generated dataset and dropping columns with null values.\n",
    "\n",
    "dfvideos = pd.read_csv(\"Videos_DF1.csv\")\n",
    "dfvideos = dfvideos.dropna()\n",
    "\n",
    "dfvideos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Defining the word2vec function\n",
    "# Tokenizing the text in each column\n",
    "\n",
    "title_tokens = [str(title).split() for title in dfvideos['title']]\n",
    "description_tokens = [str(description).split() for description in dfvideos['description']]\n",
    "keywords_tokens = [str(keywords).split() for keywords in dfvideos['keywords']]\n",
    "\n",
    "# Training Word2Vec model\n",
    "word2vec_model = Word2Vec(title_tokens + description_tokens + keywords_tokens, vector_size=500, window=5, min_count=1, workers=4)\n",
    "\n",
    "def vectorize_with_word2vec(tokens, model):\n",
    "    vectors = []\n",
    "    for token_list in tokens:\n",
    "        vector = sum([model.wv[word] for word in token_list if word in model.wv])\n",
    "        vectors.append(vector)\n",
    "    return vectors\n",
    "\n",
    "\n",
    "# Applying Word2Vec to each column and add vectors to DataFrame\n",
    "dfvideos['title_vectors'] = vectorize_with_word2vec(title_tokens, word2vec_model)\n",
    "dfvideos['description_vectors'] = vectorize_with_word2vec(description_tokens, word2vec_model)\n",
    "dfvideos['keywords_vectors'] = vectorize_with_word2vec(keywords_tokens, word2vec_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Combining the text to use in TF-IDF\n",
    "\n",
    "dfvideos['combined_text'] = dfvideos['title'] + ' ' + dfvideos['description'] + ' ' + dfvideos['keywords']\n",
    "\n",
    "dfvideos['combined_text'].fillna('', inplace=True)\n",
    "\n",
    "\n",
    "# Vectorizing and dropping variables that are no longer usefull\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(dfvideos['combined_text'])\n",
    "\n",
    "tfidf_csr = csr_matrix(tfidf_matrix)\n",
    "\n",
    "n_components = 5000\n",
    "\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "tfidf_reduced = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "del tfidf_matrix, tfidf_vectorizer\n",
    "\n",
    "# Adding the reduced TF-IDF vectors to my DataFrame, \n",
    "# then dropping the vectors to save space\n",
    "\n",
    "dfvideos['tfidf_vector'] = tfidf_reduced.tolist()\n",
    "\n",
    "\n",
    "del tfidf_csr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating functions to convert the views and duration (length)\n",
    "# to a usable numeric value.\n",
    "\n",
    "def convert_views(view_count):\n",
    "    if 'K' in view_count:\n",
    "        return float(view_count.replace('K', '').replace(',', '')) * 1000\n",
    "    elif 'M' in view_count:\n",
    "        return float(view_count.replace('M', '').replace(',', '')) * 1_000_000\n",
    "    elif 'B' in view_count:\n",
    "        return float(view_count.replace('B', '').replace(',', '')) * 1_000_000_000\n",
    "    else:\n",
    "        try:\n",
    "            return float(view_count.replace(',', ''))\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "\n",
    "def convert_video_length(length):\n",
    "    minutes, seconds = 0, 0\n",
    "\n",
    "    try:\n",
    "        # Extracting minutes\n",
    "        if 'M' in length:\n",
    "            minutes = int(length.split('M')[0][2:])\n",
    "\n",
    "        # Extracting seconds\n",
    "        if 'S' in length:\n",
    "            seconds = int(length.split('S')[0][-2:])\n",
    "\n",
    "        # Calculating total seconds\n",
    "        total_seconds = minutes * 60 + seconds\n",
    "        return total_seconds\n",
    "    \n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "# Applying functions to the dataframe\n",
    "dfvideos['views'] = dfvideos['views'].apply(convert_views)\n",
    "dfvideos['length'] = dfvideos['length'].apply(convert_video_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the youtube link from the channel_links column\n",
    "# Since it repeats in every entry\n",
    "\n",
    "dfvideos['channel_links'] = dfvideos['channel_links'].apply(lambda x: x[24:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting date to a usable format\n",
    "\n",
    "dfvideos['date'] = pd.to_datetime(dfvideos['date'], utc=True)\n",
    "\n",
    "dfvideos['year'] = dfvideos['date'].dt.year\n",
    "dfvideos['month'] = dfvideos['date'].dt.month\n",
    "dfvideos['day'] = dfvideos['date'].dt.day\n",
    "dfvideos['hour'] = dfvideos['date'].dt.hour\n",
    "dfvideos['minute'] = dfvideos['date'].dt.minute\n",
    "\n",
    "\n",
    "dfvideos['day_of_week'] = dfvideos['date'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "1        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...\n",
       "2        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...\n",
       "3        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "4        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "                               ...                        \n",
       "17007    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "17008    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "17009    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "17010    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "17011    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...\n",
       "Name: channel_vector, Length: 16934, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Preparing MultiLabelBinarizer, encoding the channels to\n",
    "# A useful format, for use as category.\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "channel_matrix = mlb.fit_transform(dfvideos['channel_links'].str.split())\n",
    "\n",
    "dfvideos['channel_vector'] = channel_matrix.tolist()\n",
    "\n",
    "dfvideos['channel_vector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['combined_text'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Selecting relevant numeric features for use in regression\u001b[39;00m\n\u001b[1;32m      3\u001b[0m features \u001b[38;5;241m=\u001b[39m dfvideos\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m9\u001b[39m:]\n\u001b[0;32m----> 5\u001b[0m X \u001b[38;5;241m=\u001b[39m dfvideos\u001b[38;5;241m.\u001b[39mdropna()[features]\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined_text\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m y \u001b[38;5;241m=\u001b[39m dfvideos\u001b[38;5;241m.\u001b[39mdropna()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mviews\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:5399\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5251\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   5252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m   5253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5260\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5261\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5262\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5263\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5264\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5397\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5398\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[1;32m   5400\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[1;32m   5401\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   5402\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[1;32m   5403\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[1;32m   5404\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[1;32m   5405\u001b[0m         inplace\u001b[38;5;241m=\u001b[39minplace,\n\u001b[1;32m   5406\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m   5407\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py:4505\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4503\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4505\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_drop_axis(labels, axis, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4508\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py:4546\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4544\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4546\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4547\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4549\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4550\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:6934\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   6933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 6934\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6935\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   6936\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['combined_text'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# Selecting relevant numeric features for use in regression\n",
    "\n",
    "features = dfvideos.columns[9:]\n",
    "\n",
    "X = dfvideos.dropna()[features].drop(columns='combined_text')\n",
    "X = X.reset_index(drop=True)\n",
    "y = dfvideos.dropna()[\"views\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating the vectors to the X variable, necessary\n",
    "# for use in the regression algorithm\n",
    "\n",
    "vector_columns = ['title_vectors', 'description_vectors', 'keywords_vectors', 'channel_vector', 'tfidf_vector']\n",
    "\n",
    "for col in vector_columns:\n",
    "    print(col)\n",
    "    rows = pd.DataFrame(list(X[col].values))\n",
    "    rows.columns = rows.columns.astype(str) + \"_\" + col\n",
    "    X = pd.concat([X, rows], axis= 1)\n",
    "\n",
    "X.drop(columns= vector_columns, inplace= True)\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scaling features, in case its needed.\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# First trying ridge regression.\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = Ridge(alpha=2.5)\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "mse = r2_score(y_test, y_pred)\n",
    "print(f'r² score: {np.sqrt(mse)}')\n",
    "print('Coefficients:', model.coef_)\n",
    "print('Intercept:', model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestRegressor\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Trying RandomForestRegressor\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X_scaled, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      8\u001b[0m rf_model \u001b[38;5;241m=\u001b[39m RandomForestRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpoisson\u001b[39m\u001b[38;5;124m\"\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     10\u001b[0m rf_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Trying RandomForestRegressor\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=200, criterion=\"poisson\", random_state=42, verbose= 2, n_jobs= 10)\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "mse = r2_score(y_test, y_pred)\n",
    "print(f'r² score: {np.sqrt(mse)}')\n",
    "print('Coefficients:', model.coef_)\n",
    "print('Intercept:', model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Trying xgboost regressor\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      8\u001b[0m xg_reg \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBRegressor(objective \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreg:squarederror\u001b[39m\u001b[38;5;124m'\u001b[39m, colsample_bytree \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.3\u001b[39m, learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m      9\u001b[0m                           max_depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m, alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, n_estimators \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m, verbosity \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     11\u001b[0m xg_reg\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# Trying xgboost regressor\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                          max_depth = 5, alpha = 10, n_estimators = 100, verbosity = 2, random_state=42)\n",
    "\n",
    "xg_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xg_reg.predict(X_test)\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R² Score (XGBoost): {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
